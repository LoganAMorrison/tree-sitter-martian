stage ADD_KEY1(
    # The key to add
    in  string key,
    # The value to add for this key.
    in  string value,
    # The file to read the initial dictionary from.
    in  json   start,
    # A file to check.  If the file exists, parse its content as a signal
    # for the job to send to itself.
    in  string failfile,
    # The output file.
    out json   result    ""  "out name",
    # The source file.
    src py     "stages/add_key",
)

stage ADD_KEY2(
    in  string key       "The key to add",
    in  string value     "The value to set the key to",
    in  json   start,
    in  string failfile  "The file to \"check\" to force failure.",
    out json   result,
    out int    very_long_output_name_should_not_push_help_text_over,
    src py     "stages/add_key",
) using (
    special  = "something\nescaped",
    volatile = strict,
)

# Adds a third key to the json in a file.
stage ADD_KEY3(
    in  string key,
    in  string value,
    in  json   start,
    in  string failfile,
    out json   result,
    out bool   disable_example,
    src py     "stages/add_key",
) retain (
    result,
)

stage ADD_KEY5(
    in  string   key,
    in  string[] value,
    out string[] value,
    src exec     "stages/whatever arg",
)

stage SUM_SQUARES(
    in  float[] values   "The values to square and then sum.",
    out float   sum,
    # default
    out int,
    src comp    "bin/sum_squares mode_arg",
) split (
    in  float   value,
    out float   square,
) using (
    # For some reason this uses lots of memory.
    mem_gb   = 4,
    # This doesn't generate files anyway.
    volatile = false,
)

stage PARAMETER_ALIGNMENT(
    in  float   longer_name_first,
    in  int[]   short_name,
    in  int     very_long_name_to_align_with,
    in  Helpful stuff,
    src comp    "thing subcommand args",
)

# Takes two files containing json dictionaries and merges them.
stage MERGE_JSON(
    in  json json1,
    in  json json2,
    out json result,
    src py   "stages/merge_json",
) using (
    # This stage does almost nothing.
    mem_gb  = 0.05,
    # Perhaps it sleeps a while.
    threads = 0.01,
)

stage MERGE_JSON2(
    in  json[] input,
    src py     "stages/merge_json",
)

stage MAP_EXAMPLE(
    in  map foo,
    src py  "stages/merge_json",
) using (
    mem_gb   = 2,
    # This stage always uses 4 threads!
    threads  = 4,
    # This stage uses 2TB of vmem.
    vmem_gb  = 1024,
    volatile = strict,
)

# This stage takes a struct input
stage STRUCT_CONSUMER(
    in  Pointalism foo,
    in  int[]      y,
    in  float[]    xarr,
    in  Point      point,
    # x coordinate
    out float      x,
    # y coordinate
    out int[]      y,
    out Point      point,
    # output file
    out json       dest   "outname.json",
    out Pointalism extra,
    out Point[]    arr,
    # The place with the source
    src comp       "stages/structy",
)

stage MAP_PRODUCER(
    in  STRUCT_CONSUMER          input,
    out map<STRUCT_CONSUMER>     flat_map,
    out map<STRUCT_CONSUMER>[]   array_of_map,
    out map<STRUCT_CONSUMER[]>   map_of_array,
    out map<STRUCT_CONSUMER[]>[] array_of_map_of_array,
    out STRUCT_CONSUMER[]        array,
    src comp                     "stages/structy",
)

stage MAP_CONSUMER(
    in  map<STRUCT_CONSUMER> foo1,
    in  map<STRUCT_CONSUMER> foo2,
    in  Point[]              point_array,
    in  map<int[]>           map_of_array,
    in  int[][]              array_2d,
    in  map<float[]>         map_of_float_array,
    in  map<float>[]         array_of_float_map,
    src comp                 "stages/structy",
)

stage PRODUCER_CONSUMER(
    in  map<STRUCT_CONSUMER>[] array_of_map,
    in  map<STRUCT_CONSUMER[]> map_of_array,
    in  int                    foo,
    src comp                   "whatever",
)

stage POINT_CONSUMER(
    in  map<Point> point,
    in  map<json>  dest,
    src comp       "whatever",
)

#
stage _HAS_DEFAULT_OUT(
    out path,
    src comp "fake",
)
